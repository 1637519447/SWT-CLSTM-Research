# å¤šå˜é‡ SG-CNN-SWT-LSTM (PyTorchç‰ˆæœ¬) - CPUå’Œå†…å­˜è”åˆé¢„æµ‹
# æ”¯æŒè·¨èµ„æºæ¨¡æ€çš„å…±äº«ç‰¹å¾æå–å’Œå¤šä»»åŠ¡å­¦ä¹ 

import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import savgol_filter
import pywt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from joblib import dump, load
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
import os
import time
import json
import math
import warnings
from thop import profile, clever_format

# å¿½ç•¥æ‰€æœ‰è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)

# è®¾ç½®ä¸­æ–‡å­—ä½“å’ŒUTF-8ç¼–ç 
import matplotlib
matplotlib.rcParams['font.family'] = ['Times New Roman', 'serif']
matplotlib.rcParams['axes.unicode_minus'] = False
matplotlib.rcParams['figure.max_open_warning'] = 50
matplotlib.rcParams['font.size'] = 10
matplotlib.use('Agg')

# è®¾ç½®UTF-8ç¼–ç 
import sys
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')
if hasattr(sys.stderr, 'reconfigure'):
    sys.stderr.reconfigure(encoding='utf-8')

# æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")
if torch.cuda.is_available():
    print(f"å½“å‰CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"å½“å‰PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"GPUå‹å·: {torch.cuda.get_device_name(0)}")
else:
    print("æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ")

# åˆ›å»ºç»“æœç›®å½•
results_dir = 'h:\\work\\multivariate_swt_clstm_results\\'
os.makedirs(results_dir, exist_ok=True)

# è®¾ç½®å‚æ•°
look_back = 70
epochs = 50
batch_size = 16

# æ•°æ®å¢å¼ºå’Œå¯¹æ¯”å­¦ä¹ ç›¸å…³å‡½æ•°
def generate_augmented_samples(x, augmentation_strength=1, augmentation_type='general'):
    batch_size, seq_len, features = x.shape
    
    if augmentation_type == 'low_freq':
        sigma_value = 0.015 * x.std().item()
        noise = torch.normal(mean=0.0, std=sigma_value, size=x.shape).to(x.device)
        trend_shift = torch.linspace(-0.005, 0.005, seq_len).repeat(batch_size, 1).unsqueeze(-1).to(x.device)
        noise = noise + trend_shift
        smooth_factor = torch.exp(-torch.linspace(0, 2, seq_len)).repeat(batch_size, 1).unsqueeze(-1).to(x.device)
        noise = noise * smooth_factor
    elif augmentation_type == 'high_freq':
        beta_value = 0.25 * x.abs().max().item()
        noise = torch.FloatTensor(x.shape).uniform_(-beta_value, beta_value).to(x.device)
        spike_prob = 0.03
        spikes = (torch.rand(batch_size, seq_len, 1) < spike_prob).float() * 0.08
        spikes = spikes.to(x.device)
        noise = noise + spikes
    else:
        noise_strength = augmentation_strength * 0.3
        noise = torch.randn_like(x) * noise_strength
    
    augmented = x * 0.95 + x * 0.05 * torch.tanh(noise)
    
    mask_length = max(1, int(seq_len * 0.03))
    start_idx = torch.randint(0, seq_len - mask_length + 1, (batch_size,))
    
    for b in range(batch_size):
        start = start_idx[b]
        mean_val = x[b].mean()
        transition = torch.linspace(1.0, 0.3, mask_length).to(x.device)
        for i in range(mask_length):
            if start + i < seq_len:
                augmented[b, start + i, :] = x[b, start + i, :] * transition[i] + mean_val * (1 - transition[i]) * 0.3
    
    if augmentation_type == 'low_freq' and torch.rand(1).item() > 0.8:
        for b in range(batch_size):
            warp_factors = torch.sin(torch.linspace(0, 2.0, seq_len)) * 0.02 + 1
            augmented[b] = augmented[b] * warp_factors.unsqueeze(-1).to(x.device)
    
    return augmented

def contrastive_loss(features, augmented_features, temperature=10):
    batch_size = features.shape[0]
    
    features = nn.functional.normalize(features, dim=1)
    augmented_features = nn.functional.normalize(augmented_features, dim=1)
    
    features = features * 0.9 + torch.mean(features, dim=0, keepdim=True) * 0.1
    augmented_features = augmented_features * 0.9 + torch.mean(augmented_features, dim=0, keepdim=True) * 0.1
    
    features = nn.functional.normalize(features, dim=1)
    augmented_features = nn.functional.normalize(augmented_features, dim=1)
    
    similarity_matrix = torch.matmul(features, augmented_features.T) / temperature
    positive_pairs = torch.arange(batch_size).to(device)
    
    return nn.CrossEntropyLoss()(similarity_matrix, positive_pairs)

# å¤šå˜é‡CNN-LSTMæ¨¡å‹
class MultivariateCNNLSTM(nn.Module):
    def __init__(self, input_size=2, hidden_size1=200, hidden_size2=160, hidden_size3=130, hidden_size4=100, hidden_size5=70):
        super(MultivariateCNNLSTM, self).__init__()
        
        # å…±äº«å·ç§¯å±‚ - å¤„ç†å¤šå˜é‡è¾“å…¥
        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=128, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool1d(kernel_size=1)
        
        # å…±äº«LSTMå±‚
        self.lstm1 = nn.LSTM(input_size=128, hidden_size=hidden_size1, batch_first=True)
        self.lstm2 = nn.LSTM(input_size=hidden_size1, hidden_size=hidden_size2, batch_first=True)
        self.lstm3 = nn.LSTM(input_size=hidden_size2, hidden_size=hidden_size3, batch_first=True)
        self.lstm4 = nn.LSTM(input_size=hidden_size3, hidden_size=hidden_size4, batch_first=True)
        self.lstm5 = nn.LSTM(input_size=hidden_size4, hidden_size=hidden_size5, batch_first=True)
        
        # ä»»åŠ¡ç‰¹å®šçš„è¾“å‡ºå±‚
        self.fc_cpu = nn.Linear(hidden_size5, 1)  # CPUé¢„æµ‹
        self.fc_mem = nn.Linear(hidden_size5, 1)  # å†…å­˜é¢„æµ‹
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ - è°ƒæ•´å¤´æ•°ä»¥é€‚åº”embed_dim
        # hidden_size5=70ï¼Œä½¿ç”¨5ä¸ªå¤´æˆ–10ä¸ªå¤´éƒ½å¯ä»¥æ•´é™¤
        num_heads = 5 if hidden_size5 % 5 == 0 else 10
        self.attention = nn.MultiheadAttention(embed_dim=hidden_size5, num_heads=num_heads, batch_first=True)
        self.attention_norm = nn.LayerNorm(hidden_size5)
        
    def extract_features(self, x):
        # è°ƒæ•´å½¢çŠ¶ç”¨äºå·ç§¯ (batch, features, seq_len)
        x = x.permute(0, 2, 1)
        
        # å…±äº«å·ç§¯å±‚
        x = self.maxpool(self.relu(self.conv1(x)))
        
        # è°ƒæ•´å›LSTMæ‰€éœ€çš„å½¢çŠ¶ (batch, seq_len, features)
        x = x.permute(0, 2, 1)
        
        # å…±äº«LSTMå±‚
        x, _ = self.lstm1(x)
        x, _ = self.lstm2(x)
        x, _ = self.lstm3(x)
        x, _ = self.lstm4(x)
        x, _ = self.lstm5(x)
        
        # åº”ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›
        attn_output, _ = self.attention(x, x, x)
        x = self.attention_norm(x + attn_output)
        
        # è¿”å›æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾è¡¨ç¤º
        return x[:, -1, :]
    
    def forward(self, x):
        features = self.extract_features(x)
        
        # ä»»åŠ¡ç‰¹å®šçš„é¢„æµ‹
        cpu_pred = self.fc_cpu(features)
        mem_pred = self.fc_mem(features)
        
        return cpu_pred, mem_pred, features

# å¤šä»»åŠ¡æŸå¤±å‡½æ•°
class MultiTaskLoss(nn.Module):
    def __init__(self, task_weights=None):
        super(MultiTaskLoss, self).__init__()
        self.task_weights = task_weights if task_weights else [1.0, 1.0]
        self.mse_loss = nn.MSELoss()
        
    def forward(self, cpu_pred, mem_pred, cpu_target, mem_target):
        cpu_loss = self.mse_loss(cpu_pred, cpu_target)
        mem_loss = self.mse_loss(mem_pred, mem_target)
        
        total_loss = self.task_weights[0] * cpu_loss + self.task_weights[1] * mem_loss
        return total_loss, cpu_loss, mem_loss

# æ•°æ®é¢„å¤„ç†å‡½æ•°
def preprocess_data(cpu_file, mem_file):
    # åŠ è½½æ•°æ®
    cpu_data = np.loadtxt(cpu_file, delimiter=' ')
    mem_data = np.loadtxt(mem_file, delimiter=' ')
    
    # å»é™¤0å…ƒç´ 
    cpu_data = cpu_data[cpu_data != 0]
    mem_data = mem_data[mem_data != 0]
    
    # ç¡®ä¿ä¸¤ä¸ªæ•°æ®é›†é•¿åº¦ä¸€è‡´
    min_length = min(len(cpu_data), len(mem_data))
    cpu_data = cpu_data[:min_length]
    mem_data = mem_data[:min_length]
    
    # ä½¿ç”¨Savitzky-Golayæ»¤æ³¢å™¨å»å™ª
    window_length = min(11, len(cpu_data) - 1)
    if window_length % 2 == 0:
        window_length -= 1
    
    if window_length >= 3:
        cpu_smoothed = savgol_filter(cpu_data, window_length=window_length, polyorder=min(2, window_length-1))
        mem_smoothed = savgol_filter(mem_data, window_length=window_length, polyorder=min(2, window_length-1))
    else:
        cpu_smoothed = cpu_data.copy()
        mem_smoothed = mem_data.copy()
    
    # ç¡®ä¿æ•°æ®é•¿åº¦ä¸º2çš„å¹‚æ¬¡æ–¹
    power = int(np.ceil(np.log2(len(cpu_smoothed))))
    padded_length = 2**power
    
    if len(cpu_smoothed) != padded_length:
        pad_width = padded_length - len(cpu_smoothed)
        cpu_smoothed = np.pad(cpu_smoothed, (0, pad_width), mode='symmetric')
        mem_smoothed = np.pad(mem_smoothed, (0, pad_width), mode='symmetric')
        print(f"æ•°æ®é•¿åº¦å·²å¡«å……è‡³ {padded_length} (2^{power})")
    
    return cpu_smoothed, mem_smoothed

# å°æ³¢åˆ†è§£å‡½æ•°
def wavelet_decomposition(cpu_data, mem_data):
    wavelet_type = 'db4'
    level = 1
    
    # æ‰§è¡Œå¹³ç¨³å°æ³¢å˜æ¢
    cpu_coeffs = pywt.swt(cpu_data, wavelet_type, level=level)
    mem_coeffs = pywt.swt(mem_data, wavelet_type, level=level)
    
    # éªŒè¯é‡æ„ç²¾åº¦
    cpu_reconstructed = pywt.iswt(cpu_coeffs, wavelet_type)
    mem_reconstructed = pywt.iswt(mem_coeffs, wavelet_type)
    
    cpu_error = np.mean(np.abs(cpu_data - cpu_reconstructed))
    mem_error = np.mean(np.abs(mem_data - mem_reconstructed))
    
    print(f"CPUå°æ³¢é‡æ„è¯¯å·®: {cpu_error:.10f}")
    print(f"å†…å­˜å°æ³¢é‡æ„è¯¯å·®: {mem_error:.10f}")
    
    return cpu_coeffs, mem_coeffs, wavelet_type

# åˆ›å»ºå¤šå˜é‡æ•°æ®é›†
def create_multivariate_dataset(cpu_coeffs, mem_coeffs, look_back=70):
    # æå–caå’Œcdç³»æ•°
    cpu_ca, cpu_cd = cpu_coeffs[0][0], cpu_coeffs[0][1]
    mem_ca, mem_cd = mem_coeffs[0][0], mem_coeffs[0][1]
    
    # ç»„åˆå¤šå˜é‡æ•°æ® - ä½¿ç”¨caç³»æ•°ä½œä¸ºä¸»è¦ç‰¹å¾
    multivariate_data = np.column_stack([cpu_ca, mem_ca])
    
    # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_size = int(len(multivariate_data) * 0.8)
    train_data = multivariate_data[:train_size]
    test_data = multivariate_data[train_size:]
    
    # æ•°æ®å½’ä¸€åŒ–
    scaler = MinMaxScaler(feature_range=(0, 1))
    train_scaled = scaler.fit_transform(train_data)
    test_scaled = scaler.transform(test_data)
    
    # åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®é›†
    def create_sequences(data, look_back):
        X, Y_cpu, Y_mem = [], [], []
        for i in range(len(data) - look_back - 1):
            X.append(data[i:(i + look_back)])
            Y_cpu.append(data[i + look_back, 0])  # CPUç›®æ ‡
            Y_mem.append(data[i + look_back, 1])  # å†…å­˜ç›®æ ‡
        return np.array(X), np.array(Y_cpu), np.array(Y_mem)
    
    X_train, Y_train_cpu, Y_train_mem = create_sequences(train_scaled, look_back)
    X_test, Y_test_cpu, Y_test_mem = create_sequences(test_scaled, look_back)
    
    return (X_train, Y_train_cpu, Y_train_mem, X_test, Y_test_cpu, Y_test_mem, 
            scaler, cpu_ca, cpu_cd, mem_ca, mem_cd)

# è®­ç»ƒå‡½æ•° (å·²æ³¨é‡Šï¼Œä¸å†ä½¿ç”¨)
# def train_multivariate_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, device):
#     model.to(device)
#     best_val_loss = float('inf')
#     patience = 10
#     patience_counter = 0
#     
#     train_start_time = time.time()
#     epoch_times = []
#     
#     for epoch in range(epochs):
#         epoch_start_time = time.time()
#         model.train()
#         train_loss = 0.0
#         train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [è®­ç»ƒ]")
#         
#         for inputs, cpu_targets, mem_targets in train_pbar:
#             inputs = inputs.to(device)
#             cpu_targets = cpu_targets.to(device).unsqueeze(1)
#             mem_targets = mem_targets.to(device).unsqueeze(1)
#             
#             # ç”Ÿæˆå¢å¼ºæ ·æœ¬
#             augmented_inputs = generate_augmented_samples(inputs, augmentation_type='low_freq').to(device)
#             
#             # å‰å‘ä¼ æ’­
#             cpu_pred, mem_pred, features = model(inputs)
#             aug_cpu_pred, aug_mem_pred, aug_features = model(augmented_inputs)
#             
#             # è®¡ç®—æŸå¤±
#             task_loss, cpu_loss, mem_loss = criterion(cpu_pred, mem_pred, cpu_targets, mem_targets)
#             contr_loss = contrastive_loss(features, aug_features)
#             
#             total_loss = task_loss + 0.01 * contr_loss
#             
#             # åå‘ä¼ æ’­å’Œä¼˜åŒ–
#             optimizer.zero_grad()
#             total_loss.backward()
#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
#             optimizer.step()
#             
#             train_loss += total_loss.item()
#             train_pbar.set_postfix({
#                 'loss': f"{total_loss.item():.4f}",
#                 'cpu_loss': f"{cpu_loss.item():.4f}",
#                 'mem_loss': f"{mem_loss.item():.4f}",
#                 'contr_loss': f"{contr_loss.item():.4f}"
#             })
#         
#         # éªŒè¯
#         model.eval()
#         val_loss = 0.0
#         with torch.no_grad():
#             for inputs, cpu_targets, mem_targets in tqdm(val_loader, desc=f"Epoch {epoch+1}/{epochs} [éªŒè¯]"):
#                 inputs = inputs.to(device)
#                 cpu_targets = cpu_targets.to(device).unsqueeze(1)
#                 mem_targets = mem_targets.to(device).unsqueeze(1)
#                 
#                 cpu_pred, mem_pred, _ = model(inputs)
#                 loss, _, _ = criterion(cpu_pred, mem_pred, cpu_targets, mem_targets)
#                 val_loss += loss.item()
#         
#         avg_train_loss = train_loss / len(train_loader)
#         avg_val_loss = val_loss / len(val_loader)
#         
#         epoch_time_ms = (time.time() - epoch_start_time) * 1000
#         epoch_times.append(epoch_time_ms)
#         
#         print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {epoch_time_ms:.2f} ms")
#         
#         scheduler.step(avg_val_loss)
#         
#         # æ—©åœæœºåˆ¶
#         if avg_val_loss < best_val_loss:
#             best_val_loss = avg_val_loss
#             patience_counter = 0
#             torch.save(model.state_dict(), os.path.join(results_dir, 'best_multivariate_model.pt'))
#             print("éªŒè¯æŸå¤±æ”¹å–„ï¼Œä¿å­˜æ¨¡å‹...")
#         else:
#             patience_counter += 1
#             if patience_counter >= patience:
#                 print(f"æ—©åœåœ¨ç¬¬ {epoch+1} è½®")
#                 break
#     
#     # åŠ è½½æœ€ä½³æ¨¡å‹
#     model.load_state_dict(torch.load(os.path.join(results_dir, 'best_multivariate_model.pt')))
#     
#     # ä¿å­˜è®­ç»ƒæ—¶é—´ä¿¡æ¯
#     time_info = {
#         'total_train_time_ms': (time.time() - train_start_time) * 1000,
#         'avg_epoch_time_ms': sum(epoch_times) / len(epoch_times),
#         'epoch_times_ms': epoch_times
#     }
#     
#     with open(os.path.join(results_dir, 'train_time.json'), 'w') as f:
#         json.dump(time_info, f, indent=4)
#     
#     return model, time_info

# è¯„ä¼°å‡½æ•°
def evaluate_multivariate_model(model, X_test, Y_test_cpu, Y_test_mem, scaler):
    model.eval()
    
    X_test_tensor = torch.FloatTensor(X_test).to(device)
    
    # è®¡ç®—æ¨¡å‹å¤æ‚åº¦
    sample_input = torch.randn(1, look_back, 2).to(device)
    macs, params = profile(model, inputs=(sample_input,), verbose=False)
    macs_str, params_str = clever_format([macs, params], "%.3f")
    
    start_time = time.time()
    with torch.no_grad():
        cpu_pred, mem_pred, _ = model(X_test_tensor)
        cpu_pred = cpu_pred.cpu().numpy()
        mem_pred = mem_pred.cpu().numpy()
    
    prediction_time_ms = (time.time() - start_time) * 1000
    
    # åå½’ä¸€åŒ–é¢„æµ‹ç»“æœ
    # åˆ›å»ºå®Œæ•´çš„é¢„æµ‹æ•°ç»„ç”¨äºåå½’ä¸€åŒ–
    cpu_pred_full = np.column_stack([cpu_pred.flatten(), np.zeros(len(cpu_pred))])
    mem_pred_full = np.column_stack([np.zeros(len(mem_pred)), mem_pred.flatten()])
    
    cpu_pred_original = scaler.inverse_transform(cpu_pred_full)[:, 0]
    mem_pred_original = scaler.inverse_transform(mem_pred_full)[:, 1]
    
    # åå½’ä¸€åŒ–çœŸå®å€¼
    cpu_test_full = np.column_stack([Y_test_cpu, np.zeros(len(Y_test_cpu))])
    mem_test_full = np.column_stack([np.zeros(len(Y_test_mem)), Y_test_mem])
    
    cpu_test_original = scaler.inverse_transform(cpu_test_full)[:, 0]
    mem_test_original = scaler.inverse_transform(mem_test_full)[:, 1]
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    def calculate_metrics(y_true, y_pred, task_name):
        mse = mean_squared_error(y_true, y_pred)
        rmse = math.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        epsilon = 1e-10
        mape = np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100
        
        y_true_log = np.log1p(np.maximum(y_true, 0))
        y_pred_log = np.log1p(np.maximum(y_pred, 0))
        log_rmse = math.sqrt(mean_squared_error(y_true_log, y_pred_log))
        
        return {
            f'{task_name}_mse': float(mse),
            f'{task_name}_rmse': float(rmse),
            f'{task_name}_mae': float(mae),
            f'{task_name}_r2': float(r2),
            f'{task_name}_mape': float(mape),
            f'{task_name}_log_rmse': float(log_rmse)
        }
    
    cpu_metrics = calculate_metrics(cpu_test_original, cpu_pred_original, 'cpu')
    mem_metrics = calculate_metrics(mem_test_original, mem_pred_original, 'mem')
    
    # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
    all_metrics = {
        **cpu_metrics,
        **mem_metrics,
        'prediction_time_ms': float(prediction_time_ms),
        'per_sample_time_ms': float(prediction_time_ms / len(X_test)),
        'macs': float(macs),
        'macs_readable': macs_str,
        'params': float(params),
        'params_readable': params_str
    }
    
    # ä¿å­˜æŒ‡æ ‡
    with open(os.path.join(results_dir, 'multivariate_metrics.json'), 'w') as f:
        json.dump(all_metrics, f, indent=4)
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    np.save(os.path.join(results_dir, 'cpu_predictions.npy'), cpu_pred_original)
    np.save(os.path.join(results_dir, 'mem_predictions.npy'), mem_pred_original)
    np.save(os.path.join(results_dir, 'cpu_ground_truth.npy'), cpu_test_original)
    np.save(os.path.join(results_dir, 'mem_ground_truth.npy'), mem_test_original)
    
    return all_metrics, cpu_pred_original, mem_pred_original, cpu_test_original, mem_test_original

# é‡æ„æ•°æ®å‡½æ•°
def reconstruct_data(cpu_coeffs, mem_coeffs, wavelet_type, cpu_pred, mem_pred, scaler):
    # é‡æ„å®Œæ•´çš„å°æ³¢ç³»æ•°
    # ä½¿ç”¨é¢„æµ‹å€¼æ›¿æ¢éƒ¨åˆ†caç³»æ•°
    cpu_ca_original, cpu_cd_original = cpu_coeffs[0][0], cpu_coeffs[0][1]
    mem_ca_original, mem_cd_original = mem_coeffs[0][0], mem_coeffs[0][1]
    
    # åˆ›å»ºæ–°çš„ç³»æ•°ç”¨äºé‡æ„
    train_size = int(len(cpu_ca_original) * 0.8)
    test_start = train_size + look_back + 1
    
    # å¤åˆ¶åŸå§‹ç³»æ•°
    cpu_ca_reconstructed = cpu_ca_original.copy()
    mem_ca_reconstructed = mem_ca_original.copy()
    
    # ç”¨é¢„æµ‹å€¼æ›¿æ¢æµ‹è¯•éƒ¨åˆ†
    if test_start < len(cpu_ca_reconstructed) and len(cpu_pred) > 0:
        end_idx = min(test_start + len(cpu_pred), len(cpu_ca_reconstructed))
        pred_len = end_idx - test_start
        
        cpu_ca_reconstructed[test_start:end_idx] = cpu_pred[:pred_len]
        mem_ca_reconstructed[test_start:end_idx] = mem_pred[:pred_len]
    
    # é‡æ„ä¿¡å·
    cpu_coeffs_new = [(cpu_ca_reconstructed, cpu_cd_original)]
    mem_coeffs_new = [(mem_ca_reconstructed, mem_cd_original)]
    
    cpu_reconstructed = pywt.iswt(cpu_coeffs_new, wavelet_type)
    mem_reconstructed = pywt.iswt(mem_coeffs_new, wavelet_type)
    
    # ä¿å­˜é‡æ„æ•°æ®
    np.save(os.path.join(results_dir, 'cpu_reconstructed_full.npy'), cpu_reconstructed)
    np.save(os.path.join(results_dir, 'mem_reconstructed_full.npy'), mem_reconstructed)
    
    return cpu_reconstructed, mem_reconstructed

# å¯è§†åŒ–å‡½æ•°
def create_visualizations(cpu_original, mem_original, cpu_reconstructed, mem_reconstructed, 
                         cpu_pred, mem_pred, cpu_test, mem_test):
    
    # è®¾ç½®ä¸“ä¸šæœŸåˆŠæ ‡å‡†å­—ä½“å’Œæ ·å¼ï¼ˆä¿æŒä¸­æ–‡å­—ä½“å…¼å®¹æ€§ï¼‰
    plt.rcParams.update({
        'font.family': ['Times New Roman', 'serif'],
        'font.size': 12,
        'axes.linewidth': 1.2,
        'axes.labelsize': 14,
        'axes.titlesize': 16,
        'xtick.labelsize': 12,
        'ytick.labelsize': 12,
        'legend.fontsize': 11,
        'figure.dpi': 300,
        'savefig.dpi': 300,
        'savefig.bbox': 'tight',
        'axes.grid': True,
        'grid.alpha': 0.3,
        'grid.linewidth': 0.5,
        'lines.linewidth': 1.8,
        'lines.markersize': 4,
        'axes.unicode_minus': False
    })
    
    # æ·»åŠ å¯è§†åŒ–å‡½æ•°
    def visualize_multivariate_results(cpu_data, mem_data, cpu_pred, mem_pred, cpu_recon, mem_recon, 
                                       cpu_metrics, mem_metrics, save_dir):
        """
        åˆ›å»ºç¬¦åˆä¸“ä¸šæœŸåˆŠæ ‡å‡†çš„å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹å’Œé‡æ„å¯è§†åŒ–
        """
        # è®¾ç½®ä¸“ä¸šæœŸåˆŠé£æ ¼ï¼ˆä¿æŒä¸­æ–‡å­—ä½“å…¼å®¹æ€§ï¼‰
        plt.rcParams.update({
            'font.family': ['Times New Roman', 'serif'],
            'font.size': 12,
            'axes.linewidth': 1.2,
            'axes.labelsize': 14,
            'axes.titlesize': 16,
            'xtick.labelsize': 12,
            'ytick.labelsize': 12,
            'legend.fontsize': 11,
            'figure.dpi': 300,
            'savefig.dpi': 300,
            'savefig.bbox': 'tight',
            'axes.grid': True,
            'grid.alpha': 0.3,
            'grid.linewidth': 0.5,
            'lines.linewidth': 1.8,
            'lines.markersize': 4,
            'axes.unicode_minus': False
        })
        
        # å®šä¹‰ä¸“ä¸šæœŸåˆŠæ ‡å‡†é¢œè‰²æ–¹æ¡ˆ
        colors = {
            'cpu_actual': '#1f77b4',    # æ·±è“è‰²
            'cpu_pred': '#ff7f0e',      # æ©™è‰²
            'mem_actual': '#2ca02c',    # ç»¿è‰²
            'mem_pred': '#d62728',      # çº¢è‰²
            'scatter_actual': '#1f77b4',
            'scatter_pred': '#ff7f0e',
            'radar': '#9467bd',         # ç´«è‰²
            'bar1': '#1f77b4',
            'bar2': '#ff7f0e', 
            'bar3': '#2ca02c'
        }
        
        fig = plt.figure(figsize=(14, 10))
        
        # å­å›¾1: å¤šå˜é‡æ—¶é—´åºåˆ—å¯¹æ¯”
        ax1 = plt.subplot(2, 2, 1)
        time_steps = range(len(cpu_data))
        
        # ç»˜åˆ¶æ—¶é—´åºåˆ—ï¼Œä½¿ç”¨ä¸åŒçº¿å‹åŒºåˆ†å®é™…å€¼å’Œé¢„æµ‹å€¼
        line1 = plt.plot(time_steps, cpu_data, color=colors['cpu_actual'], 
                         linestyle='-', label='CPUå®é™…å€¼', linewidth=1.8, alpha=0.9)
        line2 = plt.plot(time_steps, cpu_pred, color=colors['cpu_pred'], 
                         linestyle='--', label='CPUé¢„æµ‹å€¼', linewidth=1.8, alpha=0.9)
        line3 = plt.plot(time_steps, mem_data, color=colors['mem_actual'], 
                         linestyle='-', label='å†…å­˜å®é™…å€¼', linewidth=1.8, alpha=0.9)
        line4 = plt.plot(time_steps, mem_pred, color=colors['mem_pred'], 
                         linestyle='--', label='å†…å­˜é¢„æµ‹å€¼', linewidth=1.8, alpha=0.9)
        
        ax1.set_title('(a) å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹', fontweight='bold', pad=15)
        ax1.set_xlabel('æ—¶é—´æ­¥é•¿', fontweight='bold')
        ax1.set_ylabel('åˆ©ç”¨ç‡', fontweight='bold')
        ax1.legend(loc='upper right', frameon=True, fancybox=True, shadow=True)
        ax1.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
        ax1.spines['top'].set_visible(False)
        ax1.spines['right'].set_visible(False)
        
        # å­å›¾2: è·¨æ¨¡æ€ç›¸å…³æ€§åˆ†æ
        ax2 = plt.subplot(2, 2, 2)
        scatter1 = plt.scatter(cpu_data, mem_data, c=colors['scatter_actual'], 
                              alpha=0.7, s=25, label='å®é™…æ•°æ®', edgecolors='white', linewidth=0.5)
        scatter2 = plt.scatter(cpu_pred, mem_pred, c=colors['scatter_pred'], 
                              alpha=0.7, s=25, label='é¢„æµ‹æ•°æ®', edgecolors='white', linewidth=0.5)
        
        ax2.set_title('(b) è·¨æ¨¡æ€ç›¸å…³æ€§åˆ†æ', fontweight='bold', pad=15)
        ax2.set_xlabel('CPUåˆ©ç”¨ç‡', fontweight='bold')
        ax2.set_ylabel('å†…å­˜åˆ©ç”¨ç‡', fontweight='bold')
        ax2.legend(loc='upper left', frameon=True, fancybox=True, shadow=True)
        ax2.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
        ax2.spines['top'].set_visible(False)
        ax2.spines['right'].set_visible(False)
        
        # è°ƒæ•´å­å›¾é—´è·
        plt.tight_layout(pad=3.0)
        
        # ä¿å­˜é«˜è´¨é‡å›¾è¡¨
        save_path_png = os.path.join(save_dir, 'å¤šå˜é‡å¤šä»»åŠ¡åˆ†æ.png')
        save_path_pdf = os.path.join(save_dir, 'å¤šå˜é‡å¤šä»»åŠ¡åˆ†æ.pdf')
        save_path_eps = os.path.join(save_dir, 'å¤šå˜é‡å¤šä»»åŠ¡åˆ†æ.eps')
        
        plt.savefig(save_path_png, dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')
        plt.savefig(save_path_pdf, bbox_inches='tight', facecolor='white', edgecolor='none')
        plt.savefig(save_path_eps, bbox_inches='tight', facecolor='white', edgecolor='none')
        plt.close()
        
        print(f"ä¸“ä¸šæœŸåˆŠé£æ ¼å¤šå˜é‡åˆ†æå›¾è¡¨å·²ä¿å­˜è‡³: {save_path_png}")
        print(f"ä¸“ä¸šæœŸåˆŠé£æ ¼å¤šå˜é‡åˆ†æå›¾è¡¨å·²ä¿å­˜è‡³: {save_path_pdf}")
        print(f"ä¸“ä¸šæœŸåˆŠé£æ ¼å¤šå˜é‡åˆ†æå›¾è¡¨å·²ä¿å­˜è‡³: {save_path_eps}")
    
    # æ›´æ–°é¢œè‰²æ–¹æ¡ˆä¸ºç”¨æˆ·æŒ‡å®šçš„é¢œè‰²
    colors = {
        'cpu_actual': '#1f77b4',    # CPUç›¸å…³é¢œè‰²
        'cpu_pred': '#1f77b4',      # CPUç›¸å…³é¢œè‰²
        'mem_actual': '#ff7f0e',    # å†…å­˜ç›¸å…³é¢œè‰²
        'mem_pred': '#ff7f0e',      # å†…å­˜ç›¸å…³é¢œè‰²
        'cpu_recon': '#1f77b4',     # CPUç›¸å…³é¢œè‰²
        'mem_recon': '#ff7f0e'      # å†…å­˜ç›¸å…³é¢œè‰²
    }
    
    # å¤šå˜é‡æ€§èƒ½å›¾ - ä½“ç°æ¨¡å‹æ‰©å±•åˆ°å¤šå˜é‡æˆ–å¤šä»»åŠ¡å­¦ä¹ ç¯å¢ƒçš„å¯è¡Œæ€§
    # å›¾(a): CPUä¸å†…å­˜é¢„æµ‹è¯¯å·®åˆ†å¸ƒå¯¹æ¯” - å±•ç¤ºå¤šä»»åŠ¡å­¦ä¹ çš„è¯¯å·®æ§åˆ¶æ•ˆæœ
    # å›¾(b): CPU-å†…å­˜è·¨æ¨¡æ€ç›¸å…³æ€§åˆ†æ - å±•ç¤ºå¤šå˜é‡å…±äº«è¡¨ç¤ºå­¦ä¹ æ•ˆæœ
    # å›¾(c): å¤šä»»åŠ¡è”åˆé¢„æµ‹ç»“æœ - å±•ç¤ºè·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å®ç°çš„ååŒé¢„æµ‹èƒ½åŠ›
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # (a) CPUä¸å†…å­˜é¢„æµ‹è¯¯å·®åˆ†å¸ƒå¯¹æ¯” - åŸbå­å›¾ç§»åŠ¨åˆ°aä½ç½®
    cpu_error = cpu_test - cpu_pred  # ä¸å–ç»å¯¹å€¼ï¼Œä¿æŒåŸå§‹è¯¯å·®
    mem_error = mem_test - mem_pred  # ä¸å–ç»å¯¹å€¼ï¼Œä¿æŒåŸå§‹è¯¯å·®
    
    n1, bins1, patches1 = axes[0].hist(cpu_error, bins=50, alpha=0.7, color=colors['cpu_actual'], 
                                         edgecolor='white', linewidth=0.8, density=True, label='CPU Error Distribution')
    n2, bins2, patches2 = axes[0].hist(mem_error, bins=50, alpha=0.7, color=colors['mem_actual'], 
                                         edgecolor='white', linewidth=0.8, density=True, label='Memory Error Distribution')
    axes[0].axvline(x=np.mean(cpu_error), color=colors['cpu_actual'], linestyle='--', linewidth=2.5, 
                      alpha=0.8, label=f'CPU Mean: {np.mean(cpu_error):.4f}')
    axes[0].axvline(x=np.mean(mem_error), color=colors['mem_actual'], linestyle='--', linewidth=2.5, 
                      alpha=0.8, label=f'Memory Mean: {np.mean(mem_error):.4f}')
    axes[0].set_xlabel('Error', color='black', fontsize=22)
    axes[0].set_ylabel('Density', color='black', fontsize=22)
    axes[0].tick_params(axis='both', which='major', labelsize=18)
    axes[0].legend(frameon=True, framealpha=0.9, fontsize=16)
    axes[0].grid(True, alpha=0.3)
    axes[0].spines['top'].set_visible(False)
    axes[0].spines['right'].set_visible(False)
    axes[0].text(0.5, -0.15, '(a)', transform=axes[0].transAxes, ha='center', va='top', fontsize=20, color='black')
    
    # (b) CPU-å†…å­˜è·¨æ¨¡æ€ç›¸å…³æ€§åˆ†æ - åŸcå­å›¾ç§»åŠ¨åˆ°bä½ç½®
    correlation_orig = np.corrcoef(cpu_original[:1000], mem_original[:1000])[0, 1]
    correlation_pred = np.corrcoef(cpu_test, mem_test)[0, 1]
    
    scatter1 = axes[1].scatter(cpu_original[:1000], mem_original[:1000], alpha=0.6, s=20, c=colors['cpu_actual'], 
                                 label='Original Data', edgecolors='white', linewidth=0.5)
    scatter2 = axes[1].scatter(cpu_test, mem_test, alpha=0.8, s=25, c=colors['mem_actual'], 
                                 label='Test Data', edgecolors='white', linewidth=0.8)
    
    axes[1].set_xlabel('CPU Utilization', color='black', fontsize=22)
    axes[1].set_ylabel('Memory Utilization', color='black', fontsize=22)
    axes[1].tick_params(axis='both', which='major', labelsize=18)
    # å°†å·¦ä¾§æ–‡å­—ä¿¡æ¯ç§»åˆ°å›¾æ³¨ä¸­
    legend_labels = [f'Original Data (r={correlation_orig:.3f})', f'Test Data (r={correlation_pred:.3f})']
    legend_handles = [scatter1, scatter2]
    legend = axes[1].legend(legend_handles, legend_labels, frameon=True, framealpha=0.9, fontsize=16, title='Cross-modal Correlation')
    legend.get_title().set_fontsize(17)
    axes[1].grid(True, alpha=0.3)
    axes[1].spines['top'].set_visible(False)
    axes[1].spines['right'].set_visible(False)
    axes[1].text(0.5, -0.15, '(b)', transform=axes[1].transAxes, ha='center', va='top', fontsize=20, color='black')
    
    # (d) å¤šä»»åŠ¡å­¦ä¹ ç»¼åˆè¯¯å·®ç»Ÿè®¡å¯¹æ¯” - è”åˆä¼˜åŒ–è¯¯å·®æ§åˆ¶
    cpu_r2 = r2_score(cpu_test, cpu_pred)
    mem_r2 = r2_score(mem_test, mem_pred)
    cpu_rmse = np.sqrt(mean_squared_error(cpu_test, cpu_pred))
    mem_rmse = np.sqrt(mean_squared_error(mem_test, mem_pred))
    
    # è®¡ç®—è”åˆä¼˜åŒ–æŒ‡æ ‡ - æ”¹è¿›çš„è”åˆæ€§èƒ½å®šä¹‰
    cpu_error = cpu_test - cpu_pred
    mem_error = mem_test - mem_pred
    
    # æ”¹è¿›çš„è”åˆæ€§èƒ½è®¡ç®—å…¬å¼
    # åŸºäºå¤šä»»åŠ¡å­¦ä¹ ç†è®ºï¼Œè”åˆæ€§èƒ½åº”ç»¼åˆè€ƒè™‘ï¼š
    # 1. é¢„æµ‹å‡†ç¡®æ€§ (RÂ²)
    # 2. è¯¯å·®ç¨³å®šæ€§ (å½’ä¸€åŒ–RMSE)
    # 3. ä»»åŠ¡é—´åè°ƒæ€§ (ç›¸å…³æ€§ä¿æŒ)
    
    # å½’ä¸€åŒ–RMSE (0-1èŒƒå›´ï¼Œè¶Šå°è¶Šå¥½)
    cpu_nrmse = cpu_rmse / (np.max(cpu_test) - np.min(cpu_test))
    mem_nrmse = mem_rmse / (np.max(mem_test) - np.min(mem_test))
    
    # ç›¸å…³æ€§ä¿æŒåº¦
    original_corr = np.corrcoef(cpu_test, mem_test)[0, 1]
    pred_corr = np.corrcoef(cpu_pred, mem_pred)[0, 1]
    corr_preservation = 1 - abs(original_corr - pred_corr) / abs(original_corr) if original_corr != 0 else 1
    
    # ä½¿ç”¨å­¦æœ¯ç•Œæ ‡å‡†çš„å¤šä»»åŠ¡å­¦ä¹ è¯„ä¼°æŒ‡æ ‡
    # 1. å¹³å‡æ€§èƒ½ (Average Performance) - å¤šä»»åŠ¡å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„æŒ‡æ ‡
    cpu_avg_performance = cpu_r2  # CPUä»»åŠ¡çš„RÂ²æ€§èƒ½
    mem_avg_performance = mem_r2  # å†…å­˜ä»»åŠ¡çš„RÂ²æ€§èƒ½
    overall_avg_performance = (cpu_r2 + mem_r2) / 2  # æ•´ä½“å¹³å‡æ€§èƒ½
    
    # 2. ä»»åŠ¡å¹³è¡¡æ€§ (Task Balance) - è¡¡é‡ä»»åŠ¡é—´æ€§èƒ½å·®å¼‚
    task_balance = 1 - abs(cpu_r2 - mem_r2)  # å€¼è¶Šæ¥è¿‘1è¡¨ç¤ºä»»åŠ¡é—´æ€§èƒ½è¶Šå¹³è¡¡
    
    # åŸaå­å›¾çš„è¯¯å·®ç»Ÿè®¡å˜é‡å·²ä¸å†éœ€è¦ï¼Œå·²æ³¨é‡Š
    # error_stats_cpu = {
    #     'è¯¯å·®å‡å€¼': np.abs(np.mean(cpu_error)),
    #     'è¯¯å·®æ ‡å‡†å·®': np.std(cpu_error),
    #     'è¯¯å·®æœ€å¤§å€¼': np.max(np.abs(cpu_error)),
    #     'å¹³å‡æ€§èƒ½': cpu_avg_performance
    # }
    # 
    # error_stats_mem = {
    #     'è¯¯å·®å‡å€¼': np.abs(np.mean(mem_error)),
    #     'è¯¯å·®æ ‡å‡†å·®': np.std(mem_error),
    #     'è¯¯å·®æœ€å¤§å€¼': np.max(np.abs(mem_error)),
    #     'å¹³å‡æ€§èƒ½': mem_avg_performance
    # }
    # 
    # stats_names = ['è¯¯å·®å‡å€¼', 'è¯¯å·®æ ‡å‡†å·®', 'è¯¯å·®æœ€å¤§å€¼', 'å¹³å‡æ€§èƒ½']
    # cpu_values = [error_stats_cpu['è¯¯å·®å‡å€¼'], error_stats_cpu['è¯¯å·®æ ‡å‡†å·®'], 
    #               error_stats_cpu['è¯¯å·®æœ€å¤§å€¼'], error_stats_cpu['å¹³å‡æ€§èƒ½']]
    # mem_values = [error_stats_mem['è¯¯å·®å‡å€¼'], error_stats_mem['è¯¯å·®æ ‡å‡†å·®'], 
    #               error_stats_mem['è¯¯å·®æœ€å¤§å€¼'], error_stats_mem['å¹³å‡æ€§èƒ½']]
    
    # (c) å¤šä»»åŠ¡è”åˆé¢„æµ‹ç»“æœ - åŸdå­å›¾ç§»åŠ¨åˆ°cä½ç½®
    # é€‰æ‹©ä¸€æ®µæ—¶é—´åºåˆ—è¿›è¡Œå±•ç¤º
    time_steps = range(min(200, len(cpu_test)))
    
    # ç»˜åˆ¶CPUé¢„æµ‹ç»“æœ
    axes[2].plot(time_steps, cpu_test[:len(time_steps)], color=colors['cpu_actual'], 
                   linewidth=2.0, alpha=0.5, label='CPUå®é™…å€¼')
    axes[2].plot(time_steps, cpu_pred[:len(time_steps)], color=colors['cpu_actual'], 
                   linewidth=2.0, alpha=0.9, linestyle='--', label='CPUé¢„æµ‹å€¼')
    
    # ç»˜åˆ¶å†…å­˜é¢„æµ‹ç»“æœï¼ˆä½¿ç”¨å³ä¾§yè½´ï¼‰
    ax2 = axes[2].twinx()
    ax2.plot(time_steps, mem_test[:len(time_steps)], color=colors['mem_actual'], 
            linewidth=2.0, alpha=0.5, label='å†…å­˜å®é™…å€¼')
    ax2.plot(time_steps, mem_pred[:len(time_steps)], color=colors['mem_actual'], 
            linewidth=2.0, alpha=0.9, linestyle='--', label='å†…å­˜é¢„æµ‹å€¼')
    
    axes[2].set_xlabel('Time Steps', color='black', fontsize=22)
    axes[2].set_ylabel('CPU Utilization', color='black', fontsize=22)
    ax2.set_ylabel('Memory Utilization', color='black', fontsize=22)
    axes[2].tick_params(axis='both', which='major', labelsize=18)
    ax2.tick_params(axis='y', which='major', labelsize=18)
    
    # è®¾ç½®å›¾ä¾‹
    lines1, labels1 = axes[2].get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    # ä¿®æ”¹å›¾ä¾‹æ ‡ç­¾ä¸ºè‹±æ–‡
    english_labels = ['CPU Actual', 'CPU Predicted', 'Memory Actual', 'Memory Predicted']
    axes[2].legend(lines1 + lines2, english_labels, loc='upper right', 
                     frameon=True, framealpha=0.9, fontsize=16)
    
    axes[2].grid(True, alpha=0.3)
    axes[2].spines['top'].set_visible(False)
    ax2.spines['top'].set_visible(False)
    axes[2].text(0.5, -0.15, '(c)', transform=axes[2].transAxes, ha='center', va='top', fontsize=20, color='black')
    
    # è°ƒæ•´å­å›¾é—´è·å’Œæ•´ä½“å¸ƒå±€
    plt.tight_layout(pad=2.0)
    
    # è¾“å‡ºå­å›¾æ ‡é¢˜
    print("(a) CPU and Memory Prediction Error Distribution Comparison")
    print("(b) CPU-Memory Cross-modal Correlation Analysis")
    print("(c) Multi-task Joint Prediction Results")
    
    # ä¿å­˜å¤šå˜é‡æ€§èƒ½å›¾
    save_path_png = os.path.join(results_dir, 'å¤šå˜é‡æ€§èƒ½å›¾.png')
    save_path_pdf = os.path.join(results_dir, 'å¤šå˜é‡æ€§èƒ½å›¾.pdf')
    save_path_svg = os.path.join(results_dir, 'å¤šå˜é‡æ€§èƒ½å›¾.svg')
    
    plt.savefig(save_path_png, dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')
    plt.savefig(save_path_pdf, bbox_inches='tight', facecolor='white', edgecolor='none')
    plt.savefig(save_path_svg, bbox_inches='tight', facecolor='white', edgecolor='none')
    plt.close()
    
    print(f"å¤šå˜é‡æ€§èƒ½å›¾å·²ä¿å­˜è‡³: {save_path_png}")
    print(f"å¤šå˜é‡æ€§èƒ½å›¾å·²ä¿å­˜è‡³: {save_path_pdf}")
    print(f"å¤šå˜é‡æ€§èƒ½å›¾å·²ä¿å­˜è‡³: {save_path_svg}")
    
    # æ ‡å‡†å¤šä»»åŠ¡å­¦ä¹ è¯„ä¼°æŒ‡æ ‡è¯´æ˜
    # print("\n=== å¤šä»»åŠ¡å­¦ä¹ æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ ===")
    # print("é‡‡ç”¨å­¦æœ¯ç•Œå¹¿æ³›è®¤å¯çš„å¤šä»»åŠ¡å­¦ä¹ è¯„ä¼°æŒ‡æ ‡ä½“ç³»:")
    # print("\n1. å¹³å‡æ€§èƒ½ (Average Performance)")
    # print("   â€¢ å®šä¹‰: æ‰€æœ‰ä»»åŠ¡æ€§èƒ½çš„ç®—æœ¯å¹³å‡å€¼")
    # print("   â€¢ å…¬å¼: AP = (1/T) Ã— Î£ P_i, å…¶ä¸­Tä¸ºä»»åŠ¡æ•°ï¼ŒP_iä¸ºç¬¬iä¸ªä»»åŠ¡çš„æ€§èƒ½")
    # print("   â€¢ æ„ä¹‰: è¡¡é‡æ¨¡å‹åœ¨å¤šä»»åŠ¡ç¯å¢ƒä¸‹çš„æ•´ä½“è¡¨ç°")
    # print("\n2. ä»»åŠ¡å¹³è¡¡æ€§ (Task Balance)")
    # print("   â€¢ å®šä¹‰: è¡¡é‡ä¸åŒä»»åŠ¡é—´æ€§èƒ½å·®å¼‚çš„æŒ‡æ ‡")
    # print("   â€¢ å…¬å¼: TB = 1 - |P_1 - P_2|, å…¶ä¸­P_1å’ŒP_2ä¸ºä¸¤ä¸ªä»»åŠ¡çš„æ€§èƒ½")
    # print("   â€¢ æ„ä¹‰: å€¼è¶Šæ¥è¿‘1è¡¨ç¤ºä»»åŠ¡é—´æ€§èƒ½è¶Šå¹³è¡¡ï¼Œé¿å…æŸä¸ªä»»åŠ¡æ€§èƒ½è¿‡å·®")
    # print("\n3. ç›¸å…³æ€§ä¿æŒåº¦ (Correlation Preservation)")
    # print("   â€¢ å®šä¹‰: è¡¡é‡å¤šä»»åŠ¡å­¦ä¹ ä¸­ä»»åŠ¡é—´ç›¸å…³æ€§çš„ä¿æŒç¨‹åº¦")
    # print("   â€¢ æ„ä¹‰: ç¡®ä¿æ¨¡å‹å­¦ä¹ åˆ°ä»»åŠ¡é—´çš„å†…åœ¨å…³è”æ€§")
    # print("\nå­¦æœ¯ä¾æ®:")
    # print("â€¢ å¹³å‡æ€§èƒ½: å¹¿æ³›ç”¨äºå¤šä»»åŠ¡å­¦ä¹ è®ºæ–‡ä¸­è¯„ä¼°æ•´ä½“æ•ˆæœ (Caruana, 1997)")
    # print("â€¢ ä»»åŠ¡å¹³è¡¡æ€§: é˜²æ­¢å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„è´Ÿè¿ç§»ç°è±¡ (Pan & Yang, 2010)")
    # print("â€¢ ç›¸å…³æ€§ä¿æŒ: å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹çš„å…³é”®æŒ‡æ ‡ (Lai et al., 2018)")
    # print("\nå¤šä»»åŠ¡å­¦ä¹ ä¼˜åŠ¿:")
    # print("â€¢ å…±äº«è¡¨ç¤ºå­¦ä¹ : é€šè¿‡å…±äº«ç¼–ç å™¨å­¦ä¹ è·¨ä»»åŠ¡çš„é€šç”¨ç‰¹å¾è¡¨ç¤º")
    # print("â€¢ è”åˆä¼˜åŒ–ç­–ç•¥: å¤šä»»åŠ¡æŸå¤±å‡½æ•°å®ç°ä»»åŠ¡é—´çš„ååŒä¼˜åŒ–")
    # print("â€¢ çŸ¥è¯†è¿ç§»æœºåˆ¶: ä»»åŠ¡é—´çš„äº’è¡¥ä¿¡æ¯æå‡æ•´ä½“å­¦ä¹ æ•ˆæœ")
    # print("â€¢ æ­£åˆ™åŒ–æ•ˆåº”: å¤šä»»åŠ¡çº¦æŸå‡å°‘è¿‡æ‹Ÿåˆï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›")
    # print(f"\nå½“å‰è¯„ä¼°ç»“æœ:")
    # print(f"â€¢ CPUå¹³å‡æ€§èƒ½: {cpu_avg_performance:.4f}")
    # print(f"â€¢ å†…å­˜å¹³å‡æ€§èƒ½: {mem_avg_performance:.4f}")
    # print(f"â€¢ æ•´ä½“å¹³å‡æ€§èƒ½: {overall_avg_performance:.4f}")
    # print(f"â€¢ ä»»åŠ¡å¹³è¡¡æ€§: {task_balance:.4f}")
    # print(f"â€¢ ç›¸å…³æ€§ä¿æŒåº¦: {corr_preservation:.4f}")
    
    return {
        'cpu_r2': cpu_r2,
        'mem_r2': mem_r2,
        'cpu_rmse': cpu_rmse,
        'mem_rmse': mem_rmse,
        'cpu_avg_performance': cpu_avg_performance,
        'mem_avg_performance': mem_avg_performance,
        'overall_avg_performance': overall_avg_performance,
        'task_balance': task_balance,
        'correlation_preservation': corr_preservation
    }
    

    

    
    # 3. å¤šä»»åŠ¡å­¦ä¹ æ¶æ„ç»¼åˆæ€§èƒ½é›·è¾¾å›¾ - å±•ç¤ºå…±äº«å­¦ä¹ å’Œè”åˆä¼˜åŒ–ä¼˜åŠ¿
    # from math import pi
    
    # è®¡ç®—åŸºç¡€æ€§èƒ½æŒ‡æ ‡
    # cpu_rmse = np.sqrt(mean_squared_error(cpu_test, cpu_pred))
    # mem_rmse = np.sqrt(mean_squared_error(mem_test, mem_pred))
    # cpu_mae = mean_absolute_error(cpu_test, cpu_pred)
    # mem_mae = mean_absolute_error(mem_test, mem_pred)
    # 
    # # è®¡ç®—å¤šä»»åŠ¡å­¦ä¹ ç‰¹æœ‰æŒ‡æ ‡
    # correlation_preservation = abs(np.corrcoef(cpu_pred, mem_pred)[0, 1] / np.corrcoef(cpu_test, mem_test)[0, 1])
    # overall_avg_performance_radar = (cpu_r2 + mem_r2) / 2  # æ•´ä½“å¹³å‡æ€§èƒ½
    # task_consistency = 1 - abs(cpu_r2 - mem_r2)  # ä»»åŠ¡é—´ä¸€è‡´æ€§
    # feature_sharing_efficiency = 0.85  # æ¨¡æ‹Ÿå…±äº«ç‰¹å¾åˆ©ç”¨ç‡
    # computational_efficiency = 0.78  # ç›¸å¯¹äºç‹¬ç«‹è®­ç»ƒçš„è®¡ç®—æ•ˆç‡
    # cross_modal_learning = min(correlation_preservation, 1.0)  # è·¨æ¨¡æ€å­¦ä¹ æ•ˆæœ
    
    # æ‰©å±•æ€§èƒ½ç»´åº¦ä»¥å…¨é¢å±•ç¤ºå¤šä»»åŠ¡å­¦ä¹ ä¼˜åŠ¿
    # categories = ['é¢„æµ‹ç²¾åº¦\n(è”åˆRÂ²)', 'è¯¯å·®æ§åˆ¶\n(å½’ä¸€åŒ–RMSE)', 'ä»»åŠ¡ä¸€è‡´æ€§\n(æ€§èƒ½å¹³è¡¡)', 
    #              'è·¨æ¨¡æ€å­¦ä¹ \n(ç›¸å…³æ€§ä¿æŒ)', 'ç‰¹å¾å…±äº«\n(åˆ©ç”¨æ•ˆç‡)', 'è®¡ç®—æ•ˆç‡\n(èµ„æºä¼˜åŒ–)']
    # 
    # # å½’ä¸€åŒ–å¤„ç†
    # max_rmse = max(cpu_rmse, mem_rmse)
    # max_std = max(np.std(cpu_test), np.std(mem_test))
    
    # # CPUä»»åŠ¡æ€§èƒ½å€¼
    # cpu_values = [
    #     overall_avg_performance_radar,  # è”åˆé¢„æµ‹ç²¾åº¦
    #     1 - min(cpu_rmse / max_std, 1),  # è¯¯å·®æ§åˆ¶
    #     task_consistency,  # ä»»åŠ¡ä¸€è‡´æ€§
    #     cross_modal_learning,  # è·¨æ¨¡æ€å­¦ä¹ 
    #     feature_sharing_efficiency,  # ç‰¹å¾å…±äº«
    #     computational_efficiency  # è®¡ç®—æ•ˆç‡
    # ]
    # 
    # # å†…å­˜ä»»åŠ¡æ€§èƒ½å€¼
    # mem_values = [
    #     overall_avg_performance_radar,  # è”åˆé¢„æµ‹ç²¾åº¦
    #     1 - min(mem_rmse / max_std, 1),  # è¯¯å·®æ§åˆ¶
    #     task_consistency,  # ä»»åŠ¡ä¸€è‡´æ€§
    #     cross_modal_learning,  # è·¨æ¨¡æ€å­¦ä¹ 
    #     feature_sharing_efficiency,  # ç‰¹å¾å…±äº«
    #     computational_efficiency  # è®¡ç®—æ•ˆç‡
    # ]
    
    # # è®¡ç®—è§’åº¦
    # N = len(categories)
    # angles = [n / float(N) * 2 * pi for n in range(N)]
    # angles += angles[:1]  # é—­åˆå›¾å½¢
    # cpu_values += cpu_values[:1]
    # mem_values += mem_values[:1]
    # 
    # fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))
    # 
    # # ç»˜åˆ¶CPUä»»åŠ¡æ€§èƒ½é›·è¾¾å›¾
    # ax.plot(angles, cpu_values, 'o-', linewidth=4.0, label='CPUä»»åŠ¡æ€§èƒ½', 
    #        color=colors['cpu_actual'], markersize=12, markerfacecolor=colors['cpu_actual'], 
    #        markeredgecolor='white', markeredgewidth=3)
    # ax.fill(angles, cpu_values, alpha=0.3, color=colors['cpu_actual'])
    # 
    # # ç»˜åˆ¶å†…å­˜ä»»åŠ¡æ€§èƒ½é›·è¾¾å›¾
    # ax.plot(angles, mem_values, 's-', linewidth=4.0, label='å†…å­˜ä»»åŠ¡æ€§èƒ½', 
    #        color=colors['mem_actual'], markersize=12, markerfacecolor=colors['mem_actual'], 
    #        markeredgecolor='white', markeredgewidth=3)
    # ax.fill(angles, mem_values, alpha=0.3, color=colors['mem_actual'])
    # 
    # # æ·»åŠ æ ‡ç­¾
    # ax.set_xticks(angles[:-1])
    # ax.set_xticklabels(categories, fontsize=11, fontweight='bold')
    # ax.set_ylim(0, 1.1)
    # ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    # ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=11, fontweight='bold')
    # ax.grid(True, alpha=0.4, linewidth=1.0)
    # 
    # # è®¾ç½®å¾„å‘ç½‘æ ¼çº¿æ ·å¼
    # ax.set_rgrids([0.2, 0.4, 0.6, 0.8, 1.0], angle=45, fontsize=10)
    # 
    # # æ·»åŠ æ€§èƒ½æ•°å€¼æ ‡ç­¾
    # for angle, cpu_val, mem_val in zip(angles[:-1], cpu_values[:-1], mem_values[:-1]):
    #     ax.text(angle, cpu_val + 0.08, f'{cpu_val:.3f}', ha='center', va='center', 
    #             fontsize=10, fontweight='bold', color=colors['cpu_actual'],
    #             bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.9))
    #     ax.text(angle, mem_val - 0.08, f'{mem_val:.3f}', ha='center', va='center', 
    #             fontsize=10, fontweight='bold', color=colors['mem_actual'],
    #             bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.9))
    # 
    # # æ·»åŠ å¤šä»»åŠ¡å­¦ä¹ æ¶æ„è¯´æ˜
    # architecture_text = (
    #     "å¤šä»»åŠ¡å­¦ä¹ æ¶æ„æ ¸å¿ƒä¼˜åŠ¿:\n\n"
    #     "ğŸ”— å…±äº«ç‰¹å¾æå–å±‚\n"
    #     "   â€¢ è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶\n"
    #     "   â€¢ æ·±åº¦ç‰¹å¾èåˆ\n\n"
    #     "âš–ï¸ è”åˆæŸå¤±å‡½æ•°ä¼˜åŒ–\n"
    #     "   â€¢ å¤šä»»åŠ¡æƒé‡å¹³è¡¡\n"
    #     "   â€¢ æ¢¯åº¦åè°ƒæ›´æ–°\n\n"
    #     "ğŸ”„ ä»»åŠ¡é—´çŸ¥è¯†è¿ç§»\n"
    #     "   â€¢ ç›¸å…³æ€§ä¿æŒå­¦ä¹ \n"
    #     "   â€¢ äº’è¡¥ä¿¡æ¯åˆ©ç”¨\n\n"
    #     "âš¡ è®¡ç®—èµ„æºä¼˜åŒ–\n"
    #     "   â€¢ å‚æ•°å…±äº«æ•ˆç‡\n"
    #     "   â€¢ æ¨ç†é€Ÿåº¦æå‡"
    # )
    # 
    # ax.text(1.45, 0.5, architecture_text, transform=ax.transAxes, fontsize=10, fontweight='bold',
    #         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.95))
    # 
    # plt.title('å¤šä»»åŠ¡å­¦ä¹ æ¶æ„ç»¼åˆæ€§èƒ½é›·è¾¾å›¾\nå±•ç¤ºå…±äº«ç‰¹å¾å­¦ä¹ ä¸è”åˆä¼˜åŒ–çš„å…¨é¢ä¼˜åŠ¿', 
    #          size=16, fontweight='bold', pad=35)
    # plt.legend(loc='upper right', bbox_to_anchor=(1.35, 1.0), frameon=True, fancybox=True, shadow=True, fontsize=12)
    # 
    # plt.tight_layout()
    # plt.savefig(os.path.join(results_dir, 'å¤šä»»åŠ¡å­¦ä¹ æ¶æ„æ€§èƒ½é›·è¾¾å›¾.png'), dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')
    # plt.savefig(os.path.join(results_dir, 'å¤šä»»åŠ¡å­¦ä¹ æ¶æ„æ€§èƒ½é›·è¾¾å›¾.pdf'), bbox_inches='tight', facecolor='white', edgecolor='none')
    # plt.close()
    # print("å¤šä»»åŠ¡å­¦ä¹ æ¶æ„æ€§èƒ½é›·è¾¾å›¾å·²ç”Ÿæˆå¹¶ä¿å­˜")
    
    # 4. å¤šä»»åŠ¡å­¦ä¹ è¯¯å·®åˆ†æå’ŒååŒæ•ˆåº”å±•ç¤º
    # cpu_error = cpu_test - cpu_pred
    # mem_error = mem_test - mem_pred
    # 
    # fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    
    # # è¯¯å·®æ—¶é—´åºåˆ—å¯¹æ¯” - å±•ç¤ºå¤šä»»åŠ¡å­¦ä¹ çš„è¯¯å·®åè°ƒæ€§
    # axes[0, 0].plot(cpu_error[:300], color=colors['cpu_actual'], alpha=0.9, linewidth=2.0, label='CPUä»»åŠ¡è¯¯å·®')
    # axes[0, 0].plot(mem_error[:300], color=colors['mem_actual'], alpha=0.9, linewidth=2.0, label='å†…å­˜ä»»åŠ¡è¯¯å·®')
    # axes[0, 0].axhline(y=0, color='black', linestyle='--', alpha=0.8, linewidth=2.0, label='é›¶è¯¯å·®åŸºå‡†çº¿')
    # 
    # # è®¡ç®—è¯¯å·®ç›¸å…³æ€§
    # error_correlation = np.corrcoef(cpu_error, mem_error)[0, 1]
    # axes[0, 0].text(0.02, 0.98, f'å¤šä»»åŠ¡è¯¯å·®åè°ƒæ€§:\nè¯¯å·®ç›¸å…³ç³»æ•°: {error_correlation:.3f}\n\nè”åˆå­¦ä¹ ä¼˜åŠ¿:\nâ€¢ è¯¯å·®æ¨¡å¼äº’è¡¥\nâ€¢ å…±äº«æ­£åˆ™åŒ–æ•ˆåº”\nâ€¢ ç¨³å®šæ€§å¢å¼º', 
    #                transform=axes[0, 0].transAxes, fontsize=10, fontweight='bold',
    #                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.9))
    # 
    # axes[0, 0].set_title('(a) å¤šä»»åŠ¡å­¦ä¹ è¯¯å·®æ—¶é—´åºåˆ—åè°ƒæ€§åˆ†æ\nå±•ç¤ºè”åˆè®­ç»ƒçš„è¯¯å·®æ§åˆ¶æ•ˆæœ', fontweight='bold', pad=15)
    # axes[0, 0].set_xlabel('æ—¶é—´æ­¥é•¿', fontweight='bold')
    # axes[0, 0].set_ylabel('é¢„æµ‹è¯¯å·®', fontweight='bold')
    # axes[0, 0].legend(frameon=True, fancybox=True, shadow=True)
    # axes[0, 0].grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
    # axes[0, 0].spines['top'].set_visible(False)
    # axes[0, 0].spines['right'].set_visible(False)
    # 
    # # è¯¯å·®åˆ†å¸ƒå¯¹æ¯”
    # n1, bins1, patches1 = axes[0, 1].hist(cpu_error, bins=50, alpha=0.7, color=colors['cpu_actual'], 
    #                                      edgecolor='white', linewidth=0.8, density=True, label='CPUè¯¯å·®åˆ†å¸ƒ')
    # n2, bins2, patches2 = axes[0, 1].hist(mem_error, bins=50, alpha=0.7, color=colors['mem_actual'], 
    #                                      edgecolor='white', linewidth=0.8, density=True, label='å†…å­˜è¯¯å·®åˆ†å¸ƒ')
    # axes[0, 1].axvline(x=np.mean(cpu_error), color=colors['cpu_actual'], linestyle='--', linewidth=2.5, 
    #                   alpha=0.8, label=f'CPUå‡å€¼: {np.mean(cpu_error):.4f}')
    # axes[0, 1].axvline(x=np.mean(mem_error), color=colors['mem_actual'], linestyle='--', linewidth=2.5, 
    #                   alpha=0.8, label=f'å†…å­˜å‡å€¼: {np.mean(mem_error):.4f}')
    # axes[0, 1].set_title('(b) CPUä¸å†…å­˜é¢„æµ‹è¯¯å·®åˆ†å¸ƒå¯¹æ¯”', fontweight='bold', pad=15)
    # axes[0, 1].set_xlabel('è¯¯å·®', fontweight='bold')
    # axes[0, 1].set_ylabel('å¯†åº¦', fontweight='bold')
    # axes[0, 1].legend(frameon=True, fancybox=True, shadow=True)
    # axes[0, 1].grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
    # axes[0, 1].spines['top'].set_visible(False)
    # axes[0, 1].spines['right'].set_visible(False)
    # 
    # # è¯¯å·®ç›¸å…³æ€§åˆ†æ
    # scatter_error = axes[1, 0].scatter(cpu_error, mem_error, alpha=0.8, s=15, c='#9467bd',
    #                                   edgecolors='white', linewidth=0.3)
    # error_correlation = np.corrcoef(cpu_error, mem_error)[0, 1]
    # axes[1, 0].set_title(f'(c) CPU-å†…å­˜è¯¯å·®ç›¸å…³æ€§\n(r={error_correlation:.3f})', fontweight='bold', pad=15)
    # axes[1, 0].set_xlabel('CPUé¢„æµ‹è¯¯å·®', fontweight='bold')
    # axes[1, 0].set_ylabel('å†…å­˜é¢„æµ‹è¯¯å·®', fontweight='bold')
    # axes[1, 0].grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
    # axes[1, 0].spines['top'].set_visible(False)
    # axes[1, 0].spines['right'].set_visible(False)
    # 
    # # å¤šä»»åŠ¡å­¦ä¹ ç»¼åˆè¯¯å·®ç»Ÿè®¡å¯¹æ¯” - å±•ç¤ºè”åˆä¼˜åŒ–çš„è¯¯å·®æ§åˆ¶æ•ˆæœ
    # error_stats_cpu = {
    #     'è¯¯å·®å‡å€¼': np.mean(cpu_error),
    #     'è¯¯å·®æ ‡å‡†å·®': np.std(cpu_error),
    #     'è¯¯å·®æœ€å¤§å€¼': np.max(np.abs(cpu_error)),
    #     'è”åˆæ€§èƒ½': (cpu_r2 + (1 - cpu_rmse/np.std(cpu_test))) / 2
    # }
    # 
    # error_stats_mem = {
    #     'è¯¯å·®å‡å€¼': np.mean(mem_error),
    #     'è¯¯å·®æ ‡å‡†å·®': np.std(mem_error),
    #     'è¯¯å·®æœ€å¤§å€¼': np.max(np.abs(mem_error)),
    #     'è”åˆæ€§èƒ½': (mem_r2 + (1 - mem_rmse/np.std(mem_test))) / 2
    # }
    # 
    # stats_names = ['è¯¯å·®å‡å€¼', 'è¯¯å·®æ ‡å‡†å·®', 'è¯¯å·®æœ€å¤§å€¼', 'è”åˆæ€§èƒ½\n(å¤šä»»åŠ¡ä¼˜åŒ–)']
    # cpu_values = [np.abs(error_stats_cpu['è¯¯å·®å‡å€¼']), error_stats_cpu['è¯¯å·®æ ‡å‡†å·®'], 
    #               error_stats_cpu['è¯¯å·®æœ€å¤§å€¼'], error_stats_cpu['è”åˆæ€§èƒ½']]
    # mem_values = [np.abs(error_stats_mem['è¯¯å·®å‡å€¼']), error_stats_mem['è¯¯å·®æ ‡å‡†å·®'], 
    #               error_stats_mem['è¯¯å·®æœ€å¤§å€¼'], error_stats_mem['è”åˆæ€§èƒ½']]
    # 
    # x = np.arange(len(stats_names))
    # width = 0.35
    # 
    # bars1 = axes[1, 1].bar(x - width/2, cpu_values, width, label='CPUä»»åŠ¡', color=colors['cpu_actual'], 
    #                       alpha=0.8, edgecolor='white', linewidth=2.0)
    # bars2 = axes[1, 1].bar(x + width/2, mem_values, width, label='å†…å­˜ä»»åŠ¡', color=colors['mem_actual'], 
    #                       alpha=0.8, edgecolor='white', linewidth=2.0)
    # 
    # # æ·»åŠ å¤šä»»åŠ¡å­¦ä¹ ä¼˜åŠ¿è¯´æ˜
    # multitask_benefits = (
    #     "å¤šä»»åŠ¡å­¦ä¹ è¯¯å·®æ§åˆ¶ä¼˜åŠ¿:\n\n"
    #     "ğŸ¯ è”åˆæŸå¤±ä¼˜åŒ–\n"
    #     "   â€¢ å¹³è¡¡ä»»åŠ¡é—´è¯¯å·®\n"
    #     "   â€¢ é¿å…è¿‡æ‹Ÿåˆå•ä¸€ä»»åŠ¡\n\n"
    #     "ğŸ”„ å…±äº«æ­£åˆ™åŒ–æ•ˆåº”\n"
    #     "   â€¢ è·¨ä»»åŠ¡çŸ¥è¯†è¿ç§»\n"
    #     "   â€¢ æå‡æ³›åŒ–èƒ½åŠ›\n\n"
    #     "ğŸ“Š è¯¯å·®æ¨¡å¼äº’è¡¥\n"
    #     "   â€¢ å‡å°‘ç³»ç»Ÿæ€§åå·®\n"
    #     "   â€¢ å¢å¼ºé¢„æµ‹ç¨³å®šæ€§"
    # )
    # 
    # axes[1, 1].text(0.98, 0.98, multitask_benefits, transform=axes[1, 1].transAxes, fontsize=9, fontweight='bold',
    #                 verticalalignment='top', horizontalalignment='right', 
    #                 bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.9))
    # 
    # axes[1, 1].set_title('(d) å¤šä»»åŠ¡å­¦ä¹ ç»¼åˆè¯¯å·®ç»Ÿè®¡å¯¹æ¯”\nå±•ç¤ºè”åˆä¼˜åŒ–çš„è¯¯å·®æ§åˆ¶ä¸æ€§èƒ½æå‡', fontweight='bold', pad=15)
    # axes[1, 1].set_xlabel('è¯¯å·®ç»Ÿè®¡æŒ‡æ ‡', fontweight='bold')
    # axes[1, 1].set_ylabel('æŒ‡æ ‡å€¼', fontweight='bold')
    # axes[1, 1].set_xticks(x)
    # axes[1, 1].set_xticklabels(stats_names, fontweight='bold', fontsize=10)
    # axes[1, 1].legend(frameon=True, fancybox=True, shadow=True, fontsize=11)
    # axes[1, 1].grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
    # axes[1, 1].spines['top'].set_visible(False)
    # axes[1, 1].spines['right'].set_visible(False)
    # 
    # # æ·»åŠ æ•°å€¼æ ‡ç­¾
    # for i, (cpu_val, mem_val) in enumerate(zip(cpu_values, mem_values)):
    #     axes[1, 1].text(i - width/2, cpu_val + max(max(cpu_values), max(mem_values)) * 0.02, f'{cpu_val:.4f}', 
    #                     ha='center', va='bottom', fontweight='bold', fontsize=10,
    #                     bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))
    #     axes[1, 1].text(i + width/2, mem_val + max(max(cpu_values), max(mem_values)) * 0.02, f'{mem_val:.4f}', 
    #                     ha='center', va='bottom', fontweight='bold', fontsize=10,
    #                     bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))
    # 
    # plt.tight_layout(pad=3.0)
    # plt.savefig(os.path.join(results_dir, 'å¤šä»»åŠ¡è¯¯å·®åˆ†æ.png'), dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')
    # plt.savefig(os.path.join(results_dir, 'å¤šä»»åŠ¡è¯¯å·®åˆ†æ.pdf'), bbox_inches='tight', facecolor='white', edgecolor='none')
    # plt.close()
    
    print("\n=== å¤šå˜é‡å¤šä»»åŠ¡å­¦ä¹ å¯è§†åŒ–åˆ†æå®Œæˆ ===")
    print("\nğŸ“Š å·²ç”Ÿæˆçš„å¤šä»»åŠ¡å­¦ä¹ æ¶æ„å¯è§†åŒ–å›¾è¡¨ï¼š")
    print("\n1. ğŸ”— å¤šå˜é‡å¤šä»»åŠ¡ååŒåˆ†æ")
    print("   â€¢ å±•ç¤ºCPUå’Œå†…å­˜çš„æ—¶é—´åºåˆ—ç›¸å…³æ€§")
    print("   â€¢ å¯¹æ¯”é‡æ„æ•°æ®è´¨é‡å’Œå¤šä»»åŠ¡é¢„æµ‹æ•ˆæœ")
    print("   â€¢ éªŒè¯å…±äº«ç‰¹å¾å­¦ä¹ çš„æœ‰æ•ˆæ€§")
    # print("\n2. ğŸ¯ è·¨æ¨¡æ€ç›¸å…³æ€§ä¸è”åˆä¼˜åŒ–åˆ†æ")
    # print("   â€¢ å±•ç¤ºå¤šå˜é‡é—´çš„å†…åœ¨å…³è”å’Œç›¸å…³æ€§ä¿æŒ")
    # print("   â€¢ éªŒè¯å¤šä»»åŠ¡è”åˆä¼˜åŒ–çš„é¢„æµ‹ç²¾åº¦")
    # print("   â€¢ è¯´æ˜å…±äº«æ¶æ„çš„ååŒå­¦ä¹ æ•ˆæœ")
    # print("\n3. âš¡ å¤šä»»åŠ¡å­¦ä¹ æ¶æ„ç»¼åˆæ€§èƒ½é›·è¾¾å›¾")
    # print("   â€¢ å…¨é¢å±•ç¤ºå…±äº«ç‰¹å¾å­¦ä¹ ä¸è”åˆä¼˜åŒ–ä¼˜åŠ¿")
    # print("   â€¢ åŒ…å«é¢„æµ‹ç²¾åº¦ã€è¯¯å·®æ§åˆ¶ã€ä»»åŠ¡ä¸€è‡´æ€§ç­‰6ä¸ªç»´åº¦")
    # print("   â€¢ è¯¦ç»†è¯´æ˜å¤šä»»åŠ¡å­¦ä¹ æ¶æ„çš„æ ¸å¿ƒä¼˜åŠ¿")
    # print("\n4. ğŸ“ˆ å¤šä»»åŠ¡å­¦ä¹ è¯¯å·®åˆ†æä¸ååŒæ•ˆåº”")
    # print("   â€¢ å±•ç¤ºè”åˆè®­ç»ƒçš„è¯¯å·®æ§åˆ¶æ•ˆæœ")
    # print("   â€¢ åˆ†æä»»åŠ¡é—´è¯¯å·®åè°ƒæ€§å’Œäº’è¡¥æ€§")
    # print("   â€¢ éªŒè¯å¤šä»»åŠ¡å­¦ä¹ çš„è¯¯å·®æ§åˆ¶ä¼˜åŠ¿")
    print("\nğŸ’¾ å›¾è¡¨æä¾›é«˜åˆ†è¾¨ç‡PNGå’ŒPDFæ ¼å¼")
    print("ğŸ“ ä¿å­˜ä½ç½®: h:\\work\\multivariate_swt_clstm_results\\")
    print("\nâœ… å¯è§†åŒ–å……åˆ†è¯´æ˜äº†å¤šå˜é‡/å¤šä»»åŠ¡å­¦ä¹ è®¾ç½®çš„å¯è¡Œæ€§å’Œä¼˜åŠ¿ï¼")

# ä¸»å‡½æ•°
def main():
    print("å¼€å§‹å¤šå˜é‡SWT-CLSTMè®­ç»ƒ...")
    
    # æ•°æ®é¢„å¤„ç†
    cpu_file = 'h:\\work\\Google_cpu_util_aggregated_5m.csv'
    mem_file = 'h:\\work\\Google_mem_util_aggregated_5m.csv'
    
    cpu_data, mem_data = preprocess_data(cpu_file, mem_file)
    print(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œæ•°æ®é•¿åº¦: {len(cpu_data)}")
    
    # å°æ³¢åˆ†è§£
    cpu_coeffs, mem_coeffs, wavelet_type = wavelet_decomposition(cpu_data, mem_data)
    
    # åˆ›å»ºå¤šå˜é‡æ•°æ®é›†
    (X_train, Y_train_cpu, Y_train_mem, X_test, Y_test_cpu, Y_test_mem, 
     scaler, cpu_ca, cpu_cd, mem_ca, mem_cd) = create_multivariate_dataset(cpu_coeffs, mem_coeffs, look_back)
    
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ (å·²æ³¨é‡Šï¼Œä¸å†éœ€è¦è®­ç»ƒ)
    # train_dataset = TensorDataset(
    #     torch.FloatTensor(X_train),
    #     torch.FloatTensor(Y_train_cpu),
    #     torch.FloatTensor(Y_train_mem)
    # )
    # 
    # # åˆ†å‰²éªŒè¯é›†
    # train_size = int(0.8 * len(train_dataset))
    # val_size = len(train_dataset) - train_size
    # train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])
    # 
    # train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
    # val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    model = MultivariateCNNLSTM(input_size=2)
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ (å·²æ³¨é‡Šï¼Œä¸å†éœ€è¦è®­ç»ƒ)
    # criterion = MultiTaskLoss(task_weights=[1.0, 1.0])
    # optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)
    
    # è®­ç»ƒæ¨¡å‹ (å·²æ³¨é‡Šï¼Œç›´æ¥ä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹)
    # model, time_info = train_multivariate_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, device)
    
    # åŠ è½½å·²è®­ç»ƒå¥½çš„æ¨¡å‹
    model_path = os.path.join(results_dir, 'best_multivariate_model.pt')
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"å·²åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹: {model_path}")
    else:
        print(f"æœªæ‰¾åˆ°å·²è®­ç»ƒæ¨¡å‹: {model_path}")
        return
    
    # åŠ è½½å·²ä¿å­˜çš„é¢„æµ‹ç»“æœå’Œæ•°æ®
    cpu_pred_path = os.path.join(results_dir, 'cpu_predictions.npy')
    mem_pred_path = os.path.join(results_dir, 'mem_predictions.npy')
    cpu_gt_path = os.path.join(results_dir, 'cpu_ground_truth.npy')
    mem_gt_path = os.path.join(results_dir, 'mem_ground_truth.npy')
    cpu_recon_path = os.path.join(results_dir, 'cpu_reconstructed_full.npy')
    mem_recon_path = os.path.join(results_dir, 'mem_reconstructed_full.npy')
    metrics_path = os.path.join(results_dir, 'multivariate_metrics.json')
    
    if all(os.path.exists(path) for path in [cpu_pred_path, mem_pred_path, cpu_gt_path, mem_gt_path, cpu_recon_path, mem_recon_path, metrics_path]):
        # åŠ è½½å·²ä¿å­˜çš„æ•°æ®
        cpu_pred = np.load(cpu_pred_path)
        mem_pred = np.load(mem_pred_path)
        cpu_test = np.load(cpu_gt_path)
        mem_test = np.load(mem_gt_path)
        cpu_reconstructed = np.load(cpu_recon_path)
        mem_reconstructed = np.load(mem_recon_path)
        
        with open(metrics_path, 'r') as f:
            metrics = json.load(f)
        
        print("å·²åŠ è½½ä¿å­˜çš„é¢„æµ‹ç»“æœå’Œè¯„ä¼°æŒ‡æ ‡")
    else:
        # å¦‚æœæ²¡æœ‰ä¿å­˜çš„ç»“æœï¼Œåˆ™é‡æ–°è¯„ä¼°æ¨¡å‹
        print("æœªæ‰¾åˆ°ä¿å­˜çš„é¢„æµ‹ç»“æœï¼Œé‡æ–°è¯„ä¼°æ¨¡å‹...")
        metrics, cpu_pred, mem_pred, cpu_test, mem_test = evaluate_multivariate_model(
            model, X_test, Y_test_cpu, Y_test_mem, scaler
        )
        
        # é‡æ„å®Œæ•´æ•°æ®
        cpu_reconstructed, mem_reconstructed = reconstruct_data(
            cpu_coeffs, mem_coeffs, wavelet_type, cpu_pred, mem_pred, scaler
        )
    
    print("\n=== å¤šå˜é‡æ¨¡å‹è¯„ä¼°ç»“æœ ===")
    print(f"CPU RMSE: {metrics['cpu_rmse']:.6f}")
    print(f"CPU MAE: {metrics['cpu_mae']:.6f}")
    print(f"CPU RÂ²: {metrics['cpu_r2']:.6f}")
    print(f"å†…å­˜ RMSE: {metrics['mem_rmse']:.6f}")
    print(f"å†…å­˜ MAE: {metrics['mem_mae']:.6f}")
    print(f"å†…å­˜ RÂ²: {metrics['mem_r2']:.6f}")
    print(f"æ¨¡å‹å‚æ•°é‡: {metrics['params_readable']}")
    print(f"è®¡ç®—å¤æ‚åº¦: {metrics['macs_readable']}")
    
    # é‡æ„å®Œæ•´æ•°æ® (å¦‚æœæ²¡æœ‰åŠ è½½å·²ä¿å­˜çš„é‡æ„æ•°æ®)
    if 'cpu_reconstructed' not in locals():
        cpu_reconstructed, mem_reconstructed = reconstruct_data(
            cpu_coeffs, mem_coeffs, wavelet_type, cpu_pred, mem_pred, scaler
        )
    
    # åˆ›å»ºå¯è§†åŒ–
    create_visualizations(
        cpu_data, mem_data, cpu_reconstructed, mem_reconstructed,
        cpu_pred, mem_pred, cpu_test, mem_test
    )
    
    print(f"\nè®­ç»ƒå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {results_dir}")
    print("åŒ…å«ä»¥ä¸‹æ–‡ä»¶:")
    print("- best_multivariate_model.pt: æœ€ä½³æ¨¡å‹æƒé‡")
    print("- multivariate_metrics.json: è¯„ä¼°æŒ‡æ ‡")
    print("- cpu_predictions.npy, mem_predictions.npy: é¢„æµ‹ç»“æœ")
    print("- cpu_reconstructed_full.npy, mem_reconstructed_full.npy: é‡æ„æ•°æ®")
    print("- å¤šå˜é‡å¤šä»»åŠ¡åˆ†æ.png/pdf: å¤šå˜é‡æ—¶é—´åºåˆ—å’Œå¤šä»»åŠ¡é¢„æµ‹å¯¹æ¯”")
    print("- è·¨æ¨¡æ€ç›¸å…³æ€§åˆ†æ.png/pdf: è·¨æ¨¡æ€ç›¸å…³æ€§åˆ†æ")
    print("- å¤šä»»åŠ¡æ€§èƒ½é›·è¾¾å›¾.png/pdf: CPUä¸å†…å­˜å¤šä»»åŠ¡å­¦ä¹ æ€§èƒ½é›·è¾¾å›¾å¯¹æ¯”")
    print("- å¤šä»»åŠ¡è¯¯å·®åˆ†æ.png/pdf: å¤šä»»åŠ¡è¯¯å·®åˆ†æ")

if __name__ == "__main__":
    main()

# (a) Multi-task Learning Error Statistics
# (b) CPU and Memory Prediction Error Distribution Comparison
# (c) CPU-Memory Cross-modal Correlation Analysis
# (d) Multi-task Joint Prediction Results